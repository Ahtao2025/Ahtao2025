{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.2.1 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set indices: [ 100,74,126,124,4,13,36,107,71,141,65,117,42,123,82,20,106,1,62,32,21,67,121,144,97,18,86,61,59,14,46,135,22,17,28,81,31,38,116,45,73,101,92,26,7,76,43,96,87,99,138,134,84,30,47,44,147,128,51,66,15,72,114,113,127,140,55,54,98,41,33,145,37,143,130,75,119,64,79,60,16,9,132,125,142,108,52,111,63,12,6,137,120,77,89,103,139,68,110,83,91,118,53,2,56 ]\n",
      "Test set indices: [ 131,109,70,149,69,34,24,78,48,104,105,11,94,19,85,8,58,133,40,10,148,95,49,50,27,112,88,80,129,115,29,25,57,146,35,5,23,102,136,0,3,93,39,90,122 ]\n"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset from CSV\n",
    "iris = pd.read_excel('classification iris.xlsx')\n",
    "# Extract features and target\n",
    "X = iris.iloc[:, :-1].values  # All columns except the last one are features\n",
    "y = iris.iloc[:, -1].values   # The last column is the target\n",
    "n_samples = X.shape[0]# Assume X is the feature matrix, and y is the label array\n",
    "np.random.seed(1000000)# Set a random seed for reproducibility\n",
    "# Generate an array of indices representing the data points\n",
    "indices = np.arange(n_samples)\n",
    "# Randomly shuffle the indices\n",
    "np.random.shuffle(indices)\n",
    "# Define the split ratio for test and train sets\n",
    "test_size = 0.3\n",
    "split_point = int(n_samples * test_size)\n",
    "# Split the indices into test and train sets\n",
    "test_indices = indices[:split_point]  # First 30% as test set indices\n",
    "train_indices = indices[split_point:]  # Remaining 70% as train set indices\n",
    "# get training X training y and testing X testing y\n",
    "X_train = X[train_indices]\n",
    "y_train = y[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_test = y[test_indices]\n",
    "# Output the indices as comma-separated strings respectively\n",
    "print(\"Training set indices:\", '[',','.join(map(str, train_indices)), ']')\n",
    "print(\"Test set indices:\", '[',','.join(map(str, test_indices)), ']')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2.2.2 Calculation using Standard SVM Model:\n",
      "total training error: 0.0, total testing error: 0.0,\n",
      "\n",
      "class setosa:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.2619532043088393, -0.1178789419389778, 0.0, -0.20301373333935058, -0.08513479140037283], b: 14.417251080983217,\n",
      "support vector indices: [54],\n",
      "\n",
      "class versicolor:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.037345639487186374, -0.0011978790024191862, -7.046347073054026e-05, -0.0032413196536048546, -0.0016206598268024275], b: 2.80318842405375,\n",
      "support vector indices: [49, 58],\n",
      "\n",
      "class virginica:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.66868126606591, -0.09077285838452287, -0.023904731777931953, -0.4802939296033537, -0.2944314990436008], b: 70.80503892052188,\n",
      "support vector indices: [0, 41],\n",
      "\n",
      "linear separable classes:  ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel=\"linear\", C=1e5) #create svm linear model with C = 1e5\n",
    "svm.fit(X_train, y_train) # fit the training data into our svm model\n",
    "y_pred = svm.predict(X_test) #use svm model to predict the y value of each test point\n",
    "y_pred_train = svm.predict(X_train) # the predicted value of y of the training x data\n",
    "\n",
    "train_error = 1 - accuracy_score(y_train, y_pred_train) # calculate the training error by 1-accuracy(works the same as wrong prediction/number of data because accuracy = correct prediction/number of data)\n",
    "test_error = 1 - accuracy_score(y_test, y_pred) # calculate the testing error by 1-accuracy(works the same as wrong prediction/number of data because accuracy = correct prediction/number of data)\n",
    "print(\"Q2.2.2 Calculation using Standard SVM Model:\")#title\n",
    "print(f\"total training error: {train_error}, total testing error: {test_error},\\n\")#print out total training and testing data\n",
    "\n",
    "for i, class_name in enumerate(svm.classes_): # go over each class in order to calculate the total training and testing error\n",
    "    print(f\"class {class_name.split('-')[1]}:\")# current class name\n",
    "    train_error = 1 - accuracy_score(y_train == class_name, y_pred_train == class_name)#training error for data in each class\n",
    "    test_error = 1 - accuracy_score(y_test == class_name, y_pred == class_name)#testing error for data in each class\n",
    "    print(f\"training error: {train_error}, testing error: {test_error},\") # training and testing error in current class\n",
    "    weight = svm.coef_[i] #use svm model to obtain the value of w \n",
    "    bias = svm.intercept_[i] #use sym model to obtain the value of b\n",
    "    support_vectors = svm.support_ #return indexes of support vectors\n",
    "    support_classes = [y_train[i] for i in support_vectors] # the class of current support vectors\n",
    "    sv = [v for v, c in zip(support_vectors, support_classes) if c == class_name] # use zip to create combinations of indexes and classes and filter the support vectors that belongs to current class\n",
    "    print(f\"w: {weight.tolist()}, b: {bias},\")# print w and b\n",
    "    print(f\"support vector indices: {sv},\\n\")# print support vector indices for current class\n",
    "\n",
    "linear_separable = [] # create a list for all linear separable classes\n",
    "for j in svm.classes_:# go over all classes\n",
    "    if train_error == 0 and test_error == 0: #if both training error and testing error is 0, then add this linear separable class into the list\n",
    "        linear_separable.append(j)\n",
    "\n",
    "print('linear separable classes: ', linear_separable) # print all linear separable classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2.2.3 Calculation using SVM with Slack Variables (C=0.25 x t, where t=1,...,4):\n",
      "-------------------------------------------\n",
      "C=0.25,\n",
      "total training error: 0.0, total testing error: 0.0,\n",
      "\n",
      "class setosa:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.2619532043088393, -0.1178789419389778, 0.0, -0.20301373333935058, -0.08513479140037283], b: 14.417251080983217,\n",
      "support vector indices: [54],\n",
      "slack variable: [0.0],\n",
      "\n",
      "class versicolor:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.037345639487186374, -0.0011978790024191862, -7.046347073054026e-05, -0.0032413196536048546, -0.0016206598268024275], b: 2.80318842405375,\n",
      "support vector indices: [49, 58],\n",
      "slack variable: [1.9537970010513455, 0.1636936661831878],\n",
      "\n",
      "class virginica:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.4325633890363001, -0.05871830548185142, -0.015461966578221742, -0.31069294986733254, -0.19046196657822162], b: 45.4501382250442,\n",
      "support vector indices: [0, 41],\n",
      "slack variable: [2.0000264975154565, 2.0000527190363613],\n",
      "-------------------------------------------\n",
      "C=0.5,\n",
      "total training error: 0.0, total testing error: 0.0,\n",
      "\n",
      "class setosa:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.2619532043088393, -0.1178789419389778, 0.0, -0.20301373333935058, -0.08513479140037283], b: 14.417251080983217,\n",
      "support vector indices: [54],\n",
      "slack variable: [0.0],\n",
      "\n",
      "class versicolor:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.037345639487186374, -0.0011978790024191862, -7.046347073054026e-05, -0.0032413196536048546, -0.0016206598268024275], b: 2.80318842405375,\n",
      "support vector indices: [49, 58],\n",
      "slack variable: [1.9537970010513455, 0.1636936661831878],\n",
      "\n",
      "class virginica:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.66868126606591, -0.09077285838452287, -0.023904731777931953, -0.4802939296033537, -0.2944314990436008], b: 70.80503892052188,\n",
      "support vector indices: [0, 41],\n",
      "slack variable: [2.000365900053822, 2.0003944617915437],\n",
      "-------------------------------------------\n",
      "C=0.75,\n",
      "total training error: 0.0, total testing error: 0.0,\n",
      "\n",
      "class setosa:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.2619532043088393, -0.1178789419389778, 0.0, -0.20301373333935058, -0.08513479140037283], b: 14.417251080983217,\n",
      "support vector indices: [54],\n",
      "slack variable: [0.0],\n",
      "\n",
      "class versicolor:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.037345639487186374, -0.0011978790024191862, -7.046347073054026e-05, -0.0032413196536048546, -0.0016206598268024275], b: 2.80318842405375,\n",
      "support vector indices: [49, 58],\n",
      "slack variable: [1.9537970010513455, 0.1636936661831878],\n",
      "\n",
      "class virginica:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.66868126606591, -0.09077285838452287, -0.023904731777931953, -0.4802939296033537, -0.2944314990436008], b: 70.80503892052188,\n",
      "support vector indices: [0, 41],\n",
      "slack variable: [2.000365900053822, 2.0003944617915437],\n",
      "-------------------------------------------\n",
      "C=1.0,\n",
      "total training error: 0.0, total testing error: 0.0,\n",
      "\n",
      "class setosa:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.2619532043088393, -0.1178789419389778, 0.0, -0.20301373333935058, -0.08513479140037283], b: 14.417251080983217,\n",
      "support vector indices: [54],\n",
      "slack variable: [0.0],\n",
      "\n",
      "class versicolor:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.037345639487186374, -0.0011978790024191862, -7.046347073054026e-05, -0.0032413196536048546, -0.0016206598268024275], b: 2.80318842405375,\n",
      "support vector indices: [49, 58],\n",
      "slack variable: [1.9537970010513455, 0.1636936661831878],\n",
      "\n",
      "class virginica:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "w: [-0.66868126606591, -0.09077285838452287, -0.023904731777931953, -0.4802939296033537, -0.2944314990436008], b: 70.80503892052188,\n",
      "support vector indices: [0, 41],\n",
      "slack variable: [2.000365900053822, 2.0003944617915437],\n"
     ]
    }
   ],
   "source": [
    "print(\"Q2.2.3 Calculation using SVM with Slack Variables (C=0.25 x t, where t=1,...,4):\")\n",
    "\n",
    "for t in range(1, 5):# multipliers of 0.25\n",
    "    print(\"-------------------------------------------\")\n",
    "    C = 0.25 * t # apply each multiplier to achieve different values of C\n",
    "    print(f\"C={C},\") # show current value of C\n",
    "    svm = SVC(kernel=\"linear\", C=C) #set the model linear and give our different C value into the model parameter\n",
    "    svm.fit(X_train, y_train) # fit the training data into svm model\n",
    "    y_pred = svm.predict(X_test) # predicted y of testing data\n",
    "    y_pred_train = svm.predict(X_train) # predicted y of training data\n",
    "    # Calculate errors(same as the one in 2.2.2)\n",
    "    train_error = 1 - accuracy_score(y_train, y_pred_train)\n",
    "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"total training error: {train_error}, total testing error: {test_error},\\n\")# print total training error and testing error for each value of C\n",
    "\n",
    "    for i, class_name in enumerate(svm.classes_): # go over each classes\n",
    "        print(f\"class {class_name.split('-')[1]}:\") # print put current class name\n",
    "        train_error = 1 - accuracy_score(y_train == class_name, y_pred_train == class_name) # training error of current class\n",
    "        test_error = 1 - accuracy_score(y_test == class_name, y_pred == class_name) # testing error for current class\n",
    "        print(f\"training error: {train_error}, testing error: {test_error},\") # print out training error and testing error of current class\n",
    "        weight = svm.coef_[i] # get w from svm model\n",
    "        bias = svm.intercept_[i] # get b from svm model\n",
    "        support_vectors = svm.support_ #get the indexes of support vectors\n",
    "        support_classes = [y_train[i] for i in support_vectors] # get the classes of support vectors\n",
    "        sv = [v for v, c in zip(support_vectors, support_classes) if c == class_name] # filter the support vectors that belongs to the current class\n",
    "        print(f\"w: {weight.tolist()}, b: {bias},\") # print out b\n",
    "        print(f\"support vector indices: {sv},\") # print out the index of support vectors of current class\n",
    "        y_class = (y_train == class_name).astype(int) # turn y into binary index, if it's in the current class then it's 1 otherwise it's -1\n",
    "        y_class[y_class == 0] = -1\n",
    "        x_support = X_train[sv] # get the x of current support vectors\n",
    "        y_support = y_class[sv] # get the y of current support vectors\n",
    "        fX = np.dot(x_support, weight.T) + bias # derive decision function\n",
    "        margin_distance = y_support * fX # formulae of distance from support vectors to margin\n",
    "        zeta = np.maximum(0, 1 - margin_distance) #calculate ζ\n",
    "        print(f\"slack variable: {zeta.tolist()},\") # print out the value of ζ\n",
    "        \n",
    "        # add an row of space between two classes\n",
    "        if i < 2:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q2.2.4 Calculation using SVM with Kernel Functions:\n",
      "-------------------------------------------\n",
      "(a) 2nd-order Polynomial Kernel,\n",
      "total training error: 0.0, total testing error: 0.0,\n",
      "\n",
      "class setosa:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "support vector indices: [54],\n",
      "\n",
      "class versicolor:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "support vector indices: [49, 58],\n",
      "\n",
      "class virginica:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "support vector indices: [0, 41],\n",
      "-------------------------------------------\n",
      "(b) 3rd-order Polynomial Kernel,\n",
      "total training error: 0.0, total testing error: 0.0,\n",
      "\n",
      "class setosa:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "support vector indices: [54],\n",
      "\n",
      "class versicolor:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "support vector indices: [49, 58],\n",
      "\n",
      "class virginica:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "support vector indices: [0],\n",
      "-------------------------------------------\n",
      "(c) Radial Basis Function Kernel with sigma=1,\n",
      "total training error: 0.0, total testing error: 0.022222222222222254,\n",
      "\n",
      "class setosa:\n",
      "training error: 0.0, testing error: 0.022222222222222254,\n",
      "support vector indices: [4, 5, 6, 12, 15, 17, 19, 20, 25, 29, 30, 32, 33, 34, 36, 37, 39, 43, 44, 46, 53, 54, 55, 60, 69, 70, 72, 80, 81, 89, 90, 103],\n",
      "\n",
      "class versicolor:\n",
      "training error: 0.0, testing error: 0.0,\n",
      "support vector indices: [1, 8, 10, 14, 18, 21, 24, 26, 27, 28, 35, 40, 42, 45, 47, 48, 49, 52, 58, 59, 61, 66, 67, 68, 75, 77, 78, 79, 86, 88, 93, 94, 97, 99, 100, 102, 104],\n",
      "\n",
      "class virginica:\n",
      "training error: 0.0, testing error: 0.022222222222222254,\n",
      "support vector indices: [0, 2, 3, 7, 9, 11, 13, 16, 22, 23, 31, 38, 41, 50, 51, 56, 57, 62, 63, 64, 65, 71, 73, 74, 76, 82, 83, 84, 85, 87, 91, 92, 95, 96, 98, 101],\n",
      "-------------------------------------------\n",
      "(d) Sigmoidal Kernel with sigma=1,\n",
      "total training error: 0.6476190476190475, total testing error: 0.7111111111111111,\n",
      "\n",
      "class setosa:\n",
      "training error: 0.3047619047619048, testing error: 0.4,\n",
      "support vector indices: [4, 5, 6, 12, 15, 17, 19, 20, 25, 29, 30, 32, 33, 34, 36, 37, 39, 43, 44, 46, 53, 54, 55, 60, 69, 70, 72, 80, 81, 89, 90, 103],\n",
      "\n",
      "class versicolor:\n",
      "training error: 0.6476190476190475, testing error: 0.7111111111111111,\n",
      "support vector indices: [8, 10, 14, 18, 21, 24, 26, 27, 28, 35, 40, 42, 45, 47, 48, 49, 52, 58, 59, 61, 66, 67, 68, 75, 77, 78, 79, 86, 88, 93, 94, 97, 99, 100, 102, 104],\n",
      "\n",
      "class virginica:\n",
      "training error: 0.34285714285714286, testing error: 0.3111111111111111,\n",
      "support vector indices: [0, 2, 3, 7, 9, 11, 13, 16, 22, 23, 31, 38, 41, 50, 51, 56, 57, 62, 63, 64, 65, 71, 73, 74, 76, 82, 83, 84, 85, 87, 91, 92, 95, 96, 98, 101],\n"
     ]
    }
   ],
   "source": [
    "print(\"Q2.2.4 Calculation using SVM with Kernel Functions:\")\n",
    "# set parameters to 2nd-order, 3rd-order, radial basis function kernel with sigma=1 and sigmoidal kernel with sigma =1\n",
    "parameters = [\n",
    "    dict(kernel=\"poly\", degree=2, C=1e5),\n",
    "    dict(kernel=\"poly\", degree=3, C=1e5),\n",
    "    dict(kernel=\"rbf\", gamma=0.5, C=1e5),\n",
    "    dict(kernel=\"sigmoid\", gamma=0.5, C=1e5)\n",
    "]\n",
    "# set name for each kernel\n",
    "names = [\n",
    "    \"(a) 2nd-order Polynomial Kernel,\",\n",
    "    \"(b) 3rd-order Polynomial Kernel,\",\n",
    "    \"(c) Radial Basis Function Kernel with sigma=1,\",\n",
    "    \"(d) Sigmoidal Kernel with sigma=1,\"\n",
    "]\n",
    "# go over all the combination of all parameters and all names\n",
    "for params, name in zip(parameters, names):\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(name)# print out the current name \n",
    "    svm = SVC(**params) # put our parameter into the svm model \n",
    "    svm.fit(X_train, y_train) # fit the training datas into the svm model \n",
    "    y_pred = svm.predict(X_test) # predicted y of testing x\n",
    "    y_pred_train = svm.predict(X_train) # predicted y of training x\n",
    "    # calculate training error and testing error\n",
    "    train_error = 1 - accuracy_score(y_train, y_pred_train)\n",
    "    test_error = 1 - accuracy_score(y_test, y_pred)\n",
    "    # print out the training error and total testing error\n",
    "    print(f\"total training error: {train_error}, total testing error: {test_error},\\n\")\n",
    "    # go over each class\n",
    "    for i, class_name in enumerate(svm.classes_):\n",
    "        print(f\"class {class_name.split('-')[1]}:\") # print out current class name\n",
    "        train_error = 1 - accuracy_score(y_train == class_name, y_pred_train == class_name) # calculate training error \n",
    "        test_error = 1 - accuracy_score(y_test == class_name, y_pred == class_name) # calculate testing error\n",
    "        print(f\"training error: {train_error}, testing error: {test_error},\") # print out training error and testing error\n",
    "        support_vectors = svm.support_ # get index of support vectors\n",
    "        support_classes = [y_train[i] for i in support_vectors] # get the class of each support vectors\n",
    "        sv = [v for v, c in zip(support_vectors, support_classes) if c == class_name] # filter the support vector that belongs to the current class\n",
    "        print(f\"support vector indices: {sv},\") # print out filtered support vector\n",
    "\n",
    "        # add a row of space between 2 classes\n",
    "        if i < 2: print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
